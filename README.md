# Agent-robotic-foundation-model

## üë©‚Äçüíª Maintainers / Contributors

| Name | Role | Affiliation |
|------|------|-------------|
| **<a href="https://dr-lingxiao.github.io/" target="_blank" rel="noopener">Ling Xiao</a>** | Principal Investigator / Project Lead | <a href="https://www.global.hokudai.ac.jp/" target="_blank" rel="noopener">Hokkaido University</a> |
| **Xinyu Zhang** | D1 / Multimodal Navigation | <a href="https://www.global.hokudai.ac.jp/" target="_blank" rel="noopener">Hokkaido University</a> |
| **Tomohito Kawabata** | B4 / Multimodal Navigation | <a href="https://www.global.hokudai.ac.jp/" target="_blank" rel="noopener">Hokkaido University</a> |
| **Zishuo Wang** | Research Student / Multimodal Navigation | <a href="https://www.global.hokudai.ac.jp/" target="_blank" rel="noopener">Hokkaido University</a> |
| **Zhuonan Liu** | Research Student / Multimodal Navigation | <a href="https://www.global.hokudai.ac.jp/" target="_blank" rel="noopener">Hokkaido University</a> |


## 1. Scene Perception & Human Intention Understanding
### üìÑ Representative Papers
[1]  Chen, Y., Zhang, I. G., Zhang, Y., Xu, H., Zhi, P., Li, Q., & Huang, S. (2025, May). *Synergai: Perception alignment for human-robot collaboration.* [[ICRA]](https://ieeexplore.ieee.org/abstract/document/11128658/) <br>
[2]  Xu, W., Zhou, T., Zhang, T., Li, J., Chen, P., Pan, J., & Liu, X. (2025). *Exploring Grounding Abilities in Vision-Language Models through Contextual Perception*. [[IEEE Transactions on Cognitive and Developmental Systems]](https://ieeexplore.ieee.org/abstract/document/10985830) <br>
[3]  Musumeci, E., Brienza, M., Argenziano, F., Drid, A. H., Suriani, V., Nardi, D., & Bloisi, D. D. (2025). *Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning.* [[arXiv]](https://arxiv.org/abs/2506.15828) <br>
[4] Ling, L., Lin, C. H., Lin, T. Y., Ding, Y., Zeng, Y., Sheng, Y., ... & Li, Z. (2025). *Scenethesis: A language and vision agentic framework for 3d scene generation.* [[arXiv]](https://arxiv.org/abs/2505.02836) <br>
[5] Zheng, H., Tian, B., Wu, M., Tang, Z., Nahrstedt, K., & Schwing, A. (2025). *Spatio-temporal llm: Reasoning about environments and actions.* [[arXiv]](https://arxiv.org/abs/2507.05258)<br>
[6] Zhi, H., Chen, P., Li, J., Ma, S., Sun, X., Xiang, T., ... & Gan, C. (2025). *Lscenellm: Enhancing large 3d scene understanding using adaptive visual preferences.* [[CVPR]](https://openaccess.thecvf.com/content/CVPR2025/html/Zhi_LSceneLLM_Enhancing_Large_3D_Scene_Understanding_Using_Adaptive_Visual_Preferences_CVPR_2025_paper.html)<br>
[7] Gurunathan, T. S., Raza, M. S., Janakiraman, A. K., Khan, M. A., Pal, B., & Gangopadhyay, A. (2025, May). *Edge LLMs for Real-Time Contextual Understanding with Ground Robots.* [[AAAI-SS]](https://ojs.aaai.org/index.php/AAAI-SS/article/view/35583)


---

## 2. LLM-guided Socially Compliant Navigation.
### üìÑ Representative Papers
[1] Payandeh, A., Song, D., Nazeri, M., Liang, J., Mukherjee, P., Raj, A. H., ... & Xiao, X. (2024). *Social-LLaVA: Enhancing robot navigation through human-language reasoning in social spaces.* [[arXiv]](https://arxiv.org/abs/2501.09024) <br>
[2] Kong, Y., Song, D., Liang, J., Manocha, D., Yao, Z., & Xiao, X. (2024). *AutoSpatial: Visual-Language Reasoning for Social Robot Navigation through Efficient Spatial Reasoning Learning.* [[arXiv]](https://arxiv.org/abs/2503.07557) <br>
[3] Munje, M. J., Tang, C., Liu, S., Hu, Z., Zhu, Y., Cui, J., ... & Stone, P. (2025). *Socialnav-SUB: Benchmarking VLMs for scene understanding in social robot navigation.* [[arXiv]](https://arxiv.org/abs/2509.08757) <br>
[4] Francis, A., P√©rez-d‚ÄôArpino, C., Li, C., Xia, F., Alahi, A., Alami, R., ... & Mart√≠n-Mart√≠n, R. (2025). *Principles and guidelines for evaluating social robot navigation algorithms.* [[ACM Transactions on Human-Robot Interaction]](https://dl.acm.org/doi/full/10.1145/3700599) <br>
[5] Narasimhan, S., Tan, A. H., Choi, D., & Nejat, G. (2025, May). *Olivia-nav: An online lifelong vision language approach for mobile robot social navigation.* [[ICRA]](https://ieeexplore.ieee.org/abstract/document/11128004) <br>
[6] Zhu, W., Raju, A., Shamsah, A., Wu, A., Hutchinson, S., & Zhao, Y. (2025). *EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots with Deep Reinforcement Learning.* [[arXiv]](https://arxiv.org/abs/2503.12538)


---

## 3. Human‚ÄìRobot Interaction (HRI)
### üìÑ Representative Papers
[1] Han, L., Min, H., Hwangbo, G., Choi, J., & Seo, P. H. (2025). *DialNav: Multi-turn Dialog Navigation with a Remote Guide.* [[arXiv]](https://arxiv.org/abs/2509.12894)  <br>
[2] Lai, Y., Yuan, S., Zhang, B., Kiefer, B., Li, P., Deng, T., & Zell, A. (2025). *Fam-hri: Foundation-model assisted multi-modal human-robot interaction combining gaze and speech.* [[arXiv]](https://arxiv.org/abs/2503.16492)  <br>
[3] Lai, Y., Yuan, S., Nassar, Y., Fan, M., Weber, T., & R√§tsch, M. (2025). *NVP-HRI: zero shot natural voice and posture-based human‚Äìrobot interaction via large language model.* [[Expert systems with applications]](https://www.sciencedirect.com/science/article/pii/S0957417424032275)  <br>
[4] Irfan, B., Miniota, J., Thunberg, S., Lagerstedt, E., Kuoppam√§ki, S., Skantze, G., & Pereira, A. (2025). *Human-robot interaction conversational user enjoyment scale (hri cues).* [[IEEE Transactions on Affective Computing]](https://ieeexplore.ieee.org/abstract/document/11084909)  <br>
[5] Skantze, G., & Irfan, B. (2025, March). *Applying general turn-taking models to conversational human-robot interaction.* [[HRI]](https://ieeexplore.ieee.org/abstract/document/10973958) <br>
[6] Koubaa, A., Ammar, A., & Boulila, W. (2025). *Next‚Äêgeneration human‚Äêrobot interaction with ChatGPT and robot operating system.* [[Software: Practice and Experience]](https://onlinelibrary.wiley.com/doi/full/10.1002/spe.3377)  <br>
[7] Mende, M., Shneiderman, B., & Boccanfuso, L. (2025). *Commentary: the future of human-robot interactions.* [[Journal of Service Research]](https://journals.sagepub.com/doi/full/10.1177/10946705241296041)  <br>
[8] Macalupu, V., Miller, E., Martin, L., & Caldwell, G. (2025). *Human‚Äìrobot interactions and experiences of staff and service robots in aged care.* [[Scientific Reports]](https://www.nature.com/articles/s41598-025-86255-w)   <br>
[9] Lawrence, S., Jouaiti, M., Hoey, J., Nehaniv, C. L., & Dautenhahn, K. (2025). *The Role of Social Norms in Human‚ÄìRobot Interaction: A Systematic Review.* [[ACM Transactions on Human-Robot Interaction]](https://dl.acm.org/doi/full/10.1145/3722120)  <br>
[10] Irfan, B., & Skantze, G. (2025, March). *Between you and me: Ethics of self-disclosure in human-robot interaction.* [[HRI]](https://ieeexplore.ieee.org/abstract/document/10974215)   <br>
[11] Asuzu, K., Singh, H., & Idrissi, M. (2025). *Human‚Äìrobot interaction through joint robot planning with large language models.* [[Intelligent Service Robotics]](https://link.springer.com/article/10.1007/s11370-024-00570-1)  <br>
[12] Ayub, A., De Francesco, Z., Holthaus, P., Nehaniv, C. L., & Dautenhahn, K. (2025). *Continual learning through human-robot interaction: Human perceptions of a continual learning robot in repeated interactions.* [[International Journal of Social Robotics]](https://link.springer.com/article/10.1007/s12369-025-01214-9) <br>
[13] Roy, L., Croft, E. A., Ramirez, A., & Kuliƒá, D. (2025). *GPT-Driven Gestures: Leveraging Large Language Models to Generate Expressive Robot Motion for Enhanced Human-Robot Interaction.* [[RAL]](https://ieeexplore.ieee.org/abstract/document/10909198) <br>
[14] Sasabuchi, K., Wake, N., Kanehira, A., Takamatsu, J., & Ikeuchi, K. (2025). *Agreeing to interact in human-robot interaction using large language models and vision language models.* [[arXiv]](https://arxiv.org/abs/2503.15491)   <br>
[15] Moorman, N., Zhao, M., Luebbers, M. B., Van Waveren, S., Simmons, R., Admoni, H., ... & Gombolay, M. (2025). *Bi-Directional Mental Model Reconciliation for Human-Robot Interaction with Large Language Models.* [[arXiv]](https://arxiv.org/abs/2503.07547)  <br>
[16] Lai, Y., Yuan, S., Nassar, Y., Fan, M., Gopal, A., Yorita, A., ... & R√§tsch, M. (2025). *NMM-HRI: Natural Multi-modal Human-Robot Interaction with Voice and Deictic Posture via Large Language Model.* [[arXiv]](https://ui.adsabs.harvard.edu/abs/2025arXiv250100785L/abstract)
  
---

## 4. Open-world Navigation
### üìÑ Representative Papers
[1] Shah, D., Osinski, B., Ichter, B., & Levine, S. (2022). *LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action.* [[arXiv]](https://arxiv.org/abs/2501.09024) <br>
[2] Xiao, J., Sun, Y., Shao, Y., Gan, B., Liu, R., Wu, Y., ... & Deng, X. (2025, October). *Uav-on: A benchmark for open-world object goal navigation with aerial agents.* [[ACM Multimedia]](https://dl.acm.org/doi/abs/10.1145/3746027.3758251) <br>
[3] Bar, A., Zhou, G., Tran, D., Darrell, T., & LeCun, Y. (2025). *Navigation world models.* [[CVPR]](https://openaccess.thecvf.com/content/CVPR2025/html/Bar_Navigation_World_Models_CVPR_2025_paper.html) <br>
[4] Ziliotto, F., Campari, T., Serafini, L., & Ballan, L. (2025). *TANGO: training-free embodied AI agents for open-world tasks.* [[CVPR]](https://openaccess.thecvf.com/content/CVPR2025/html/Ziliotto_TANGO_Training-free_Embodied_AI_Agents_for_Open-world_Tasks_CVPR_2025_paper.html) <br>
[5] Liu, X., Li, J., Deng, Y., Chen, R., Zhang, Y., Ma, Y., ... & Feng, C. (2025). *Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI.* [[arXiv]](https://arxiv.org/abs/2511.20620) <br>
[6] Xue, X., Hu, J., Luo, M., Shichao, X., Chen, J., Xie, Z., ... & Chu, Z. (2025). *OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation.* [[arXiv]](https://arxiv.org/abs/2509.25687) <br>
[7] Cruz, S., Doctor, K., Funk, C., & Scheirer, W. (2025). *Open issues in open world learning.* [[AI Magazine]](https://onlinelibrary.wiley.com/doi/full/10.1002/aaai.70001) <br>
[8] Busch, F. L., Homberger, T., Ortega-Peimbert, J., Yang, Q., & Andersson, O. (2025, May). One map to find them all: Real-time open-vocabulary mapping for zero-shot multi-object navigation. [[ICRA]](https://ieeexplore.ieee.org/abstract/document/11128393) <br>
[9] Wang, K., Lu, L., Liu, M., Jiang, J., Li, Z., Zhang, B., ... & Shen, C. (2025). *Odyssey: Open-world quadrupeds exploration and manipulation for long-horizon tasks.* [[arXiv]](https://arxiv.org/abs/2508.08240) <br>
[10] Gao, J., Liu, R., & Wang, W. (2025). *3d gaussian map with open-set semantic grouping for vision-language navigation.* [[ICCV]](https://openaccess.thecvf.com/content/ICCV2025/html/Gao_3D_Gaussian_Map_with_Open-Set_Semantic_Grouping_for_Vision-Language_Navigation_ICCV_2025_paper.html)  <br>
[11] Chen, C., Lu, L., Yang, L., Zhang, Y., Chen, Y., Jia, R., & Pan, J. (2025). *Signage-Aware Exploration in Open World using Venue Maps.* [[RAL]](https://ieeexplore.ieee.org/abstract/document/10878474) <br>
[12] Jiang, J., Zhu, Y., Wu, Z., & Song, J. (2025). *DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing Scenes.* [[arXiv]](https://arxiv.org/abs/2506.01950)  <br>
[13] Huang, S., Shi, C., Yang, J., Dong, H., Mi, J., Li, K., ... & Wei, X. (2025). *KiteRunner: Language-Driven Cooperative Local-Global Navigation Policy with UAV Mapping in Outdoor Environments.* [[arXiv]](https://arxiv.org/abs/2503.08330) <br>
[14] Qiu, D., You, J., Gong, Z., Qiu, R., Xiong, H., & Liang, J. (2025). *SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes.* [[arXiv]](https://arxiv.org/abs/2505.18881)  <br>
[15] Yin, H., Wei, H., Xu, X., Guo, W., Zhou, J., & Lu, J. (2025). *GC-VLN: Instruction as graph constraints for training-free vision-and-language navigation.* [[arXiv]](https://arxiv.org/abs/2509.10454) <br>
[16] Han, M., Ma, L., Zhumakhanova, K., Radionova, E., Zhang, J., Chang, X., ... & Laptev, I. (2025). *Roomtour3d: Geometry-aware video-instruction tuning for embodied navigation.* [[CVPR]](https://openaccess.thecvf.com/content/CVPR2025/html/Han_RoomTour3D_Geometry-Aware_Video-Instruction_Tuning_for_Embodied_Navigation_CVPR_2025_paper.html) <br>

 
---

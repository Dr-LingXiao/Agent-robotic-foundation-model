# Agent-robotic-foundation-model

## üë©‚Äçüíª Maintainers / Contributors

| Name | Role | Affiliation |
|------|------|-------------|
| **<a href="https://dr-lingxiao.github.io/" target="_blank" rel="noopener">Ling Xiao</a>** | Principal Investigator / Project Lead | <a href="https://www.global.hokudai.ac.jp/" target="_blank" rel="noopener">Hokkaido University</a> |
| **Xinyu Zhang** | D1 / Multimodal Navigation | <a href="https://www.global.hokudai.ac.jp/" target="_blank" rel="noopener">Hokkaido University</a> |
| **Tomohito Kawabata** | B4 / Multimodal Navigation | <a href="https://www.global.hokudai.ac.jp/" target="_blank" rel="noopener">Hokkaido University</a> |
| **Zishuo Wang** | Research Student / Multimodal Navigation | <a href="https://www.global.hokudai.ac.jp/" target="_blank" rel="noopener">Hokkaido University</a> |
| **Zhuonan Liu** | Research Student / Multimodal Navigation | <a href="https://www.global.hokudai.ac.jp/" target="_blank" rel="noopener">Hokkaido University</a> |


## 1. Scene Perception & Human Intention Understanding
### üìÑ Representative Papers
[1]  Chen, Y., Zhang, I. G., Zhang, Y., Xu, H., Zhi, P., Li, Q., & Huang, S. (2025, May). *Synergai: Perception alignment for human-robot collaboration.* [[ICRA]](https://ieeexplore.ieee.org/abstract/document/11128658/) <br>
[2]  Xu, W., Zhou, T., Zhang, T., Li, J., Chen, P., Pan, J., & Liu, X. (2025). *Exploring Grounding Abilities in Vision-Language Models through Contextual Perception*. [[IEEE Transactions on Cognitive and Developmental Systems]](https://ieeexplore.ieee.org/abstract/document/10985830) <br>
[3]  Musumeci, E., Brienza, M., Argenziano, F., Drid, A. H., Suriani, V., Nardi, D., & Bloisi, D. D. (2025). *Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning.* [[arXiv]](https://arxiv.org/abs/2506.15828) <br>
[4] Ling, L., Lin, C. H., Lin, T. Y., Ding, Y., Zeng, Y., Sheng, Y., ... & Li, Z. (2025). *Scenethesis: A language and vision agentic framework for 3d scene generation.* [[arXiv]](https://arxiv.org/abs/2505.02836) <br>
[5] Zheng, H., Tian, B., Wu, M., Tang, Z., Nahrstedt, K., & Schwing, A. (2025). *Spatio-temporal llm: Reasoning about environments and actions.* [[arXiv]](https://arxiv.org/abs/2507.05258)<br>
[6] Zhi, H., Chen, P., Li, J., Ma, S., Sun, X., Xiang, T., ... & Gan, C. (2025). *Lscenellm: Enhancing large 3d scene understanding using adaptive visual preferences.* [[CVPR]](https://openaccess.thecvf.com/content/CVPR2025/html/Zhi_LSceneLLM_Enhancing_Large_3D_Scene_Understanding_Using_Adaptive_Visual_Preferences_CVPR_2025_paper.html)<br>
[7] Gurunathan, T. S., Raza, M. S., Janakiraman, A. K., Khan, M. A., Pal, B., & Gangopadhyay, A. (2025, May). *Edge LLMs for Real-Time Contextual Understanding with Ground Robots.* [[AAAI-SS]](https://ojs.aaai.org/index.php/AAAI-SS/article/view/35583)


---

## 2. LLM-guided Socially Compliant Navigation.
### üìÑ Representative Papers
[1] Payandeh, A., Song, D., Nazeri, M., Liang, J., Mukherjee, P., Raj, A. H., ... & Xiao, X. (2024). *Social-LLaVA: Enhancing robot navigation through human-language reasoning in social spaces.* [[arXiv]](https://arxiv.org/abs/2501.09024) <br>
[2] Kong, Y., Song, D., Liang, J., Manocha, D., Yao, Z., & Xiao, X. (2024). *AutoSpatial: Visual-Language Reasoning for Social Robot Navigation through Efficient Spatial Reasoning Learning.* [[arXiv]](https://arxiv.org/abs/2503.07557) <br>
[3] Munje, M. J., Tang, C., Liu, S., Hu, Z., Zhu, Y., Cui, J., ... & Stone, P. (2025). *Socialnav-SUB: Benchmarking VLMs for scene understanding in social robot navigation.* [[arXiv]](https://arxiv.org/abs/2509.08757) <br>
[4] Francis, A., P√©rez-d‚ÄôArpino, C., Li, C., Xia, F., Alahi, A., Alami, R., ... & Mart√≠n-Mart√≠n, R. (2025). *Principles and guidelines for evaluating social robot navigation algorithms.* [[ACM Transactions on Human-Robot Interaction]](https://dl.acm.org/doi/full/10.1145/3700599) <br>
[5] Narasimhan, S., Tan, A. H., Choi, D., & Nejat, G. (2025, May). *Olivia-nav: An online lifelong vision language approach for mobile robot social navigation.* [[ICRA]](https://ieeexplore.ieee.org/abstract/document/11128004) <br>
[6] Zhu, W., Raju, A., Shamsah, A., Wu, A., Hutchinson, S., & Zhao, Y. (2025). *EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots with Deep Reinforcement Learning.* [[arXiv]](https://arxiv.org/abs/2503.12538)


---

## 3. Human‚ÄìRobot Interaction (HRI)
### üìÑ Representative Papers
[1] Han, L., Min, H., Hwangbo, G., Choi, J., & Seo, P. H. (2025). *DialNav: Multi-turn Dialog Navigation with a Remote Guide.* [[arXiv]](https://arxiv.org/abs/2509.12894)  <br>
[2] Lai, Y., Yuan, S., Zhang, B., Kiefer, B., Li, P., Deng, T., & Zell, A. (2025). *Fam-hri: Foundation-model assisted multi-modal human-robot interaction combining gaze and speech.* [[arXiv]](https://arxiv.org/abs/2503.16492)  <br>
[3] Lai, Y., Yuan, S., Nassar, Y., Fan, M., Weber, T., & R√§tsch, M. (2025). *NVP-HRI: zero shot natural voice and posture-based human‚Äìrobot interaction via large language model.* [[Expert systems with applications]](https://www.sciencedirect.com/science/article/pii/S0957417424032275)  <br>
[4] Irfan, B., Miniota, J., Thunberg, S., Lagerstedt, E., Kuoppam√§ki, S., Skantze, G., & Pereira, A. (2025). *Human-robot interaction conversational user enjoyment scale (hri cues).* [[IEEE Transactions on Affective Computing]](https://ieeexplore.ieee.org/abstract/document/11084909)  <br>
[5] Skantze, G., & Irfan, B. (2025, March). *Applying general turn-taking models to conversational human-robot interaction.* [[HRI]](https://ieeexplore.ieee.org/abstract/document/10973958)


---

## 4. Open-world Navigation
### üìÑ Representative Papers
[1] Shah, D., Osinski, B., Ichter, B., & Levine, S. (2022). *LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action.* [[arXiv]](https://arxiv.org/abs/2501.09024) <br>
[2] Xiao, J., Sun, Y., Shao, Y., Gan, B., Liu, R., Wu, Y., ... & Deng, X. (2025, October). *Uav-on: A benchmark for open-world object goal navigation with aerial agents.* [[ACM Multimedia]](https://dl.acm.org/doi/abs/10.1145/3746027.3758251) <br>
[3] Bar, A., Zhou, G., Tran, D., Darrell, T., & LeCun, Y. (2025). *Navigation world models.* [[CVPR]](https://openaccess.thecvf.com/content/CVPR2025/html/Bar_Navigation_World_Models_CVPR_2025_paper.html) <br>
[4] Ziliotto, F., Campari, T., Serafini, L., & Ballan, L. (2025). *TANGO: training-free embodied AI agents for open-world tasks.* [[CVPR]](https://openaccess.thecvf.com/content/CVPR2025/html/Ziliotto_TANGO_Training-free_Embodied_AI_Agents_for_Open-world_Tasks_CVPR_2025_paper.html) <br>
[5] Liu, X., Li, J., Deng, Y., Chen, R., Zhang, Y., Ma, Y., ... & Feng, C. (2025). *Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI.* [[arXiv]](https://arxiv.org/abs/2511.20620) <br>
[6] Xue, X., Hu, J., Luo, M., Shichao, X., Chen, J., Xie, Z., ... & Chu, Z. (2025). *OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation.* [[arXiv]](https://arxiv.org/abs/2509.25687) <br>

---

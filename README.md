# Agent-robotic-foundation-model

## ğŸ‘©â€ğŸ’» Maintainers / Contributors

| Name | Role | Affiliation |
|------|------|-------------|
| **Ling Xiao** | Principal Investigator / Project Lead | Hokkaido University |
| **Xinyu Zhang** | D1 / Multimodal Navigation | Hokkaido University |
| **Tomohito Kawabata** | B4 / Multimodal Navigation | Hokkaido University |
| **Zishuo Wang** | Research Student / Multimodal Navigation | Hokkaido University |
| **Zhuonan Liu** | Research Student / Multimodal Navigation | Hokkaido University |


## 1. LLM-based Social Perception & Human Intention Understanding
LLMs for understanding humans, behaviors, intentions, and social context.
### ğŸ“„ Representative Papers
[1] Payandeh, A., Song, D., Nazeri, M., Liang, J., Mukherjee, P., Raj, A. H., ... & Xiao, X. (2024). *Social-LLaVA: Enhancing robot navigation through human-language reasoning in social spaces.* [[arXiv]](https://arxiv.org/abs/2501.09024) <br>
[2]  <br>
[3]  <br>


---

## 2. LLM-guided Socially Compliant Planning
Using LLMs for semantic reasoning, high-level rules, etiquette reasoning, and human-aware path planning.
### ğŸ“„ Representative Papers
[1]   <br>
[2]   <br>
[3]   <br>
[4]   <br>


---

## 3. LLM-enhanced Humanâ€“Robot Interaction (HRI)
LLMs for natural language interaction, clarification, intent communication, and explainable actions.
### ğŸ“„ Representative Papers
[1]   <br>
[2]   <br>
[3]   <br>
[4]   <br>


---

## 4. Real-World Deployment & System Challenges for LLM-based Navigation
Safety, robustness, hallucination mitigation, multi-modal fusion, and real-world deployment.
### ğŸ“„ Representative Papers
[1]  <br>
[2]  <br>
[3]  <br>
[4]  <br>


---
